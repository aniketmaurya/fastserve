{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"FastServe","text":"<p>Machine Learning Serving focused on GenAI &amp; LLMs with simplicity as the top priority.</p> <p></p> <p>YouTube: How to serve your own GPT like LLM in 1 minute with FastServe</p>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install git+https://github.com/aniketmaurya/fastserve.git@main\n</code></pre>"},{"location":"#run-locally","title":"Run locally","text":"<pre><code>python -m fastserve\n</code></pre>"},{"location":"#usageexamples","title":"Usage/Examples","text":""},{"location":"#serve-mistral-7b-with-llama-cpp","title":"Serve Mistral-7B with Llama-cpp","text":"<pre><code>from fastserve.models import ServeLlamaCpp\n\nmodel_path = \"openhermes-2-mistral-7b.Q5_K_M.gguf\"\nserve = ServeLlamaCpp(model_path=model_path, )\nserve.run_server()\n</code></pre> <p>or, run <code>python -m fastserve.models --model llama-cpp --model_path openhermes-2-mistral-7b.Q5_K_M.gguf</code> from terminal.</p>"},{"location":"#serve-sdxl-turbo","title":"Serve SDXL Turbo","text":"<pre><code>from fastserve.models import ServeSDXLTurbo\n\nserve = ServeSDXLTurbo(device=\"cuda\", batch_size=2, timeout=1)\nserve.run_server()\n</code></pre> <p>or, run <code>python -m fastserve.models --model sdxl-turbo --batch_size 2 --timeout 1</code> from terminal.</p>"},{"location":"#face-detection","title":"Face  Detection","text":"<pre><code>from fastserve.models import FaceDetection\n\nserve = FaceDetection(batch_size=2, timeout=1)\nserve.run_server()\n</code></pre> <p>or, run <code>python -m fastserve.models --model face-detection --batch_size 2 --timeout 1</code> from terminal.</p>"},{"location":"#image-classification","title":"Image Classification","text":"<pre><code>from fastserve.models import ServeImageClassification\n\napp = ServeImageClassification(\"resnet18\", timeout=1, batch_size=4)\napp.run_server()\n</code></pre> <p>or, run <code>python -m fastserve.models --model image-classification --model_name resnet18 --batch_size 4 --timeout 1</code> from terminal.</p>"},{"location":"#serve-custom-model","title":"Serve Custom Model","text":"<p>To serve a custom model, you will have to implement <code>handle</code> method for <code>FastServe</code> that processes a batch of inputs and returns the response as a list.</p> <pre><code>from fastserve import FastServe\n\n\nclass MyModelServing(FastServe):\n    def __init__(self):\n        super().__init__(batch_size=2, timeout=0.1)\n        self.model = create_model(...)\n\n    def handle(self, batch: List[BaseRequest]) -&gt; List[float]:\n        inputs = [b.request for b in batch]\n        response = self.model(inputs)\n        return response\n\n\napp = MyModelServing()\napp.run_server()\n</code></pre> <p>You can run the above script in terminal, and it will launch a FastAPI server for your custom model.</p>"},{"location":"#deploy","title":"Deploy","text":""},{"location":"#lightning-ai-studio","title":"Lightning AI Studio \u26a1\ufe0f","text":"<pre><code>python fastserve.deploy.lightning --filename main.py \\\n    --user LIGHTNING_USERNAME \\\n    --teamspace LIGHTNING_TEAMSPACE \\\n    --machine \"CPU\"  # T4, A10G or A10G_X_4\n</code></pre>"},{"location":"#contribute","title":"Contribute","text":"<p>Install in editable mode:</p> <pre><code>git clone https://github.com/aniketmaurya/fastserve.git\ncd fastserve\npip install -e .\n</code></pre> <p>Create a new branch</p> <pre><code>git checkout -b \uff1cnew-branch\uff1e\n</code></pre> <p>Make your changes, commit and create a PR.</p>"},{"location":"CHANGELOG/","title":"Release Notes","text":""},{"location":"CHANGELOG/#001","title":"0.0.1","text":"<ul> <li>Setup repo</li> </ul>"}]}